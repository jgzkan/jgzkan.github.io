---
layout: post
title: "[심화세션]_5주차"
published: true
date: 2024-11-04
math: true
categories: 
tags: KHUDA 심화세션
---




## 12. 통계 기반 분석 방법론

### 12.1 분석 모델 개요

| 통계 모델(Statistical model) | 기계 학습(Machine learning) |
|:---:|:---:|
| 모형과 해석 중요, 오차와 불확정성 강조 | 대용량 데이터 활용해 예측 정확도 향상 중시 |


![img1](assets/img/DBRS_week5/img1.png)
![img2](assets/img/DBRS_week5/img2.png)

- 비지도 학습: a.k.a. 자율학습. 변수 간의 패턴을 파악하거나 데이터를 군집화하는 방법
- 강화 학습: 동물이 시행착오를 통해 학습하는 과정(상&벌)을 기본 콘셉트로 한 방법 중 하나

<br/>

### 12.2 주성분 분석(PCA)

- 주성분 분석(Principal Component Analysis;PCA): 여러 개의 독립변수들을 잘 설명해 줄 수 있는 주된 성분을 추출하는 기법. 독립변수(차원)의 수를 줄일 수 있다. 변수들이 모두 등간 척도나 비율 척도로 측정한 양적변수여야 하고, 관측치들이 서로 독립적이고 정규분포를 이루고 있어야 사용 가능.

> 차원의 저주: 변수가 늘어남에 따라 차원이 커지면서 분석을 위한 최소한의 필요 데이터 건수가 늘어나면서 예측이 불안정해지는 문제를 말한다. 변수가 늘어날 수록 기하 급수적으로 많은 양의 데이터가 확보되어야 하고, 이럴수록 과적합의 위험성이 증가한다. 또한 다중공선성 문제도 발생할 수 있어 주의가 필요하다. 

- 공통요인분석(Common Factor Analysisl;CFA): 변수들 사이에 존재하는 차원을 규명함으로써 변수들 간의 구조를 파악하는 데 주로 사용됨

PCA와 CFA는 변수들의 잠재적인 성분을 추출하여 차원을 줄이는 방법이다. 

>PCA는 일반적으로 제1주성분, 제2주성분만으로 대부분의 설명력이 포함되기 때문에 두 개의 주성분 선정한다. 

- 주성분 분석 실습: 캐글의 "Glass Classification" 데이터셋으로 주성분 분석 실습 진행

<br/>

### 12.3 공통요인분석(CFA)

PCA와 CFA는 **요인분석(Factor Analysis;FA)**을 하기 위한 기법의 종류다. 우리가 흔히 말하는 요인분석은 말 그대로 주어진 데이터의 요인을 분석한다는 큰 개념이고, **요인분석**을 하기 위해 **전체 분산을 토대로** 요인을 추출하는 **PCA**를 사용하거나 **공통분산만**을 토대로 요인을 추출하는 **CFA**를 선택할 수 있다. 

- **탐색적 요인분석(Exploratory Factor Analysis;EFA)**은 변수와 요인 간의 관계가 사전에 정립되지 않거나 체계화되지 않은 상태에서 변수 간의 관계를 알아보기 위해 사용한다. 

- **확인적 요인분석(Confirmatory Factor Analysis;CFA)**은 이미 변수들의 속성을 예상하고 있는 상태에서 실제로 구조가 그러한지 확인하기 위한 목적으로 쓰인다. 

CFA도 전체 독립변수를 축약한다는 점에서 PCA와 동일하지만, 상관성이 높은 변수들을 묶어 잠재된 몇 개의 변수를 찾는다는 점에서 차이가 있다. 정확성 관점에서는 CFA가 PCA보다 우수한 면이 있다. 

- PCA: 모든 독립 변수드르이 총 변량(총 분산)을 기반으로 요인을 추출하기 때문에 전체 변수를 가장 잘 설명해 주는 순으로 주성분의 우위가 결정된다. 

- CFA: 변수들 간의 공통 변량(공통분산)만을 기반으로 하여 요인 추출함. 따라서 CFA로 생성한 주성분들은 서로 간에 무엇이 더 중요한 변수라는 우위 개념이 없다. 설명력에 차이는 있겠지만, 기본적으로 종류가 다른 변수를 만들어내는 것이 목적이다. 

요인분석을 하기 위해서는 우선 독립변수들 간의 상관성이 요인분석에 적합한지 검증을 해야 한다. 이를 확인하기 위한 방법으로 바틀렛(Bartlett) 테스트와 KMO(Kaiser-Meyer-Olkin) 검정이 있다. 

- 바틀렛 테스트: 행렬식을 이용하여 카이제곱값을 구하여 각 변수들 사이의 상관계수의 적합성 검증하는 방법으로 p값을 나타낸다. p<0.05면, 대각행렬이 아니라는, 즉 변수들 간에 상관관계가 있다는 뜻이므로 분석에 적합하다고 판단함. 

요인분석은 변수 그룹 간의 상관관계가 각각 달라야 요인이 다르게 분리되기 때문에 변수들의 상관관계가 모두 비슷하게 높거나 낮게 나타나면 요인분석에 부적합하다. 

일반적으로 사회과학에서는 고유치가 1 이상인 요인만 선택하며, 총 분산의 60% 이상을 설명해주는 요인까지 선정하는 것이 일반적이다. 

scree plot을 참고하여 적정한 요인의 수를 결정할 수 있고, 그래프의 경사가 낮아지는 지점을 엘보우 포인트(elbow point)라고 부르며 일반적으로 이 부분까지의 요인을 선택한다. 

- 공통요인분석 실습: 캐글의 "Nba 2020-2021 Season Player Stats" 데이터셋 사용

<br/>

### 12.4 다중공선성 해결과 섀플리 밸류 분석

> 다중공선성(multicollinearity): 독립변수들 간의 상관관계가 높은 현상, 두 개 이상의 독립변수가 서로 선형적인 관계를 나타내는 경우

다중공선성이 발생하게 되면 '독립'변수라는 가정에 위배되어 추정치의 통계적 유의성이 낮아지고, 모델의 정합성이 맞지 않는 문제가 발생한다. 즉, 회귀 모델의 유의성을 판단하기 어려워진다. 

> 다중공선성 판별 기준
1. 회귀 분석 모델링 전, 상관분석을 통해 문제의 독립변수 찾기: 상관계수의 절대치가 0.7이상이면 두 변수 간의 상관성이 높다는 뜻, 다중공선성 의심 가능 (변수가 많으면 hard)
2. 독립변수들의 설명력을 나타내는 R<sup>2<sup/>값이 크지만 회귀계수에 대한 t-value가 낮다면 의심 가능
3. VIF(Variance Inflation Factor, 분산팽창계수)를 통해 판단하기. VIF 값은 회귀분석 모델에 사용된 다른 독립 변수들이 해당 변수 대신 모델을 설명해줄 수 있는 정도를 의미함. 따라서 VIF 값이 크다면 해당 변수가 다른 변수들과 상관성이 높다는 것.  

**다중공선성을 해결하기 위한 가장 기본적인 방법은 VIF 값이 높은 변수들 중에서 종속변수와의 상관성(설명력)이 가장 낮은 변수를 제거하고 다시 VIF 값을 확인하는 과정을 반복하는 것이다.**


일반적으로 표본이 늘어날수록 분산과 표준오차가 감소하기에 표본 관측치를 추가적으로 확보하여 다중공선성을 완화시키는 것도 하나의 방법이다. 

하지만 현실적으로는, 로그를 취하거나 표준화/정규화 변환을 통해 변수를 가공하여 다른 변수와의 높았던 상관성이 완화될 수 있다. 연속형 변수를 구간화 혹은 명목변수로 변환하는 것도 가능

PCA를 통해서도 해결할 수 있지만, 주성분 변수는 유사한 변수들을 하나의 변수로 합쳐낸 효과가 있어 변수의 해석이 어려워진다는 점을 염두해야 한다. 

변수선택 알고리즘을 활용하는 방법도 있다. 보통 전진 선택법, 후진 제거법, 단계적 선택법 중 하나를 선택하여 특정 로직에 의해 모형에 적합한 변수를 자동으로 선정한다. 

![img3](assets/img/DBRS_week5/img3.png)


>섀플리 밸류(Shapley Value) 분석: 로이드 섀플리가 개발한 독립변수의 설명력 분배 방법

![img4](assets/img/DBRS_week5/img4.png)

![img5](assets/img/DBRS_week5/img5.png)

<br/>

### 12.5 데이터 마사지와 블라인드 분석

>데이터 마사지: 데이터 분석 결과가 예상하거나 의도한 방향과 다를 때 해석이 달라질 수 있도록 유도하는 것. 데이터 조작과는 차이가 있지만 분석가의 주관적 판단이 개입되는 것이기에 지양해야 한다. 
- 편향된 데이터 전처리
- 매직그래프 사용: 데이터의 의미를 온전히 담지 못하도록 주관을 담아 그래프 제작
- 분모 바꾸기 등 관점 변환
- ML 모델의 파라미터 값 변경 및 연산 반복
- 심슨의 역설


- 블라인드 분석: 데이터 마사지에 의한 왜곡을 방지하기 위해 사용하는 방법.

데이터 분석이 아무리 통계적 수치를 기반으로 한다고 해도 분석가의 주관적 판단을 완전히 배제할 수는 없다. 이를 인지적 편향 혹은 확증 편향이라 한다. 블라인드 분석은 이러한 편향에 의한 오류를 최소화한다. 

- 블라인드 분석의 사용 목적: 기존에 분석가가 중시했던 변수가 큰 의미 없는 것으로 결과가 나와도 무리해서 의미부여를 하거나 그 변수에 집착해 해석에 유리하도록 변수를 가공하게 되는 실수를 방지하는 것. 

<br/>

### 12.6 Z-test와 T-test

>Z-test, T-test는 
- 두 집단 간의 평균 차이를 분석할 때 사용하는 것이고
- 단일 표본 집단의 평균 변화를 분석하거나 두 집단의 평균값 혹은 비율 차이를 분석할 때 사용
- 분석하고자 하는 변수가 양적 변수이며, 정규 분포이며, 등분산이라는 조건이 충족돼야함

(세부 내용 생략)

![img6](assets/img/DBRS_week5/img6.png)

- Z-test, T-test 실습: 캐글 "Golf ball testing data set" 사용

<br/>

### 12.7 ANOVA(Analysis of Variance)

3집단 이상의 평균을 검정할 때 T-test를 사용할 수 있으나, 신뢰도가 하락하는 문제가 발생한다. 때문에 집단이 3개 이상이면 일반적으로 ANOVA를 사용한다. 

- 분산분석이라고도 불리는 ANOVA는 F분포를 사용한다.

- ANOVA는 회귀분석과 같이 독립변수(집단의 종류)가 종속변수(평균값의 차이 여부)에 미치는 영향을 분석하는 것이다. 

- ANOVA의 종류
|요인 1개|요인 2개|요인 3개+|
|:---:|:---:|:---:|
|일원 분산분석(one-way ANOVA)|이원 분산분석(two-way ANOVA)|N원 분산분석(N-way ANOVA)|

- 독립 변수: 범주형(분류형) 변수
- 종속 변수: 연속형 변수

> 회귀분석: 독립/종속 모두 연속형  &  교차분석: 독립/종속 모두 분류형

ANOVA는 각 집단의 평균값 차이가 통계적으로 유의한지 검증한다.
이를 **집단 내 분산**과 **집단 간 평균의 분산**으로 나타낼 수 있다.

![img7](assets/img/DBRS_week5/img7.png)

![img8](assets/img/DBRS_week5/img8.png)


- ANOVA 실습: "Golf ball testing data set" 데이터셋

<br/>

### 12.8 카이제곱 검정, Chi-square test (교차분석, Crosstabs)

- 카이제곱 검정(a.k.a. 교차분석): 명목/서열척도와 같은 범주형 변수들 간의 연관성을 분석하기 위해 결합분포를 활용하는 방법. 

- 변수들 간의 범주를 동시에 교차하는 **교차표** -> 변수 상호 간의 독립성, 관련성 분석 -> 교차분석은 상관분석과는 달리 *연관성의 정도를 수치로 표현할 수 없다.* 대신 검정 통계량 카이제곱을 통해 변수 간에 연관성이 없다는 **귀무가설을 기각하는지 여부**로 상관성이 있고 없음을 판단한다. 

(계산식 생략)

- 카이제곱 검정 실습: 캐글 "smoker" 데이터셋

<br/>

## 13. 머신러닝 분석 방법론


### 13.1 선형 회귀 분석과 Elastic Net(예측모델)

회귀분석은 결국 각 독립변수의 평균을 통해 종속변수를 예측한다. 

- 회귀분석: **"종속변수 Y의 값에 영향을 주는 독립변수 X들의 조건의 고려하여 구한 평균값"**

최적의 회귀선을 구하는 과정을 모형 적합이라 부르며, **예측치와 관측치들 간의 수직 거리(오차)의 제곱합을 최소로 하는 직선이 회귀선이 된다. -> 최소제곱추정법

회귀분석은 독립변수 간에 상관관계가 없어야 하기에 다중 회귀 분석을 할 때는 다중공선성 검사를 해야 한다. 

이외에도 잔차의 정규성/등분산성, 선형성과 같은 기본 조건을 충족해야 한다. 

회귀 분석의 회귀선이 선형으로 되어있기에, 독립변수와 종속변수가 비선형적 관계일 경우 예측력이 떨어진다. 
이럴 때 변수를 구간화하여 이항변수로 표시된 몇 개의 더미변수로 변환하여 분석하면 된다. 그러면 일정 구간에 대해 음의 관계를 갖도록 수식을 조정할 수 있다. 
또는 비선형성이 심하지 않은 경우에 로그합수를 이용하여 치환하면 비선형성을 어느 정도 완화시킬 수 있다. 

![img9](assets/img/DBRS_week5/img9.png)

> 결과 해석 시 사용되는 용어
- Parameter Estimate: 각 변수의 계수(coefficient)
- T value: 노이즈 대비 시그널의 강도, 선형관계가 얼마나 강한지 나타냄
- P value: 유의도를 나타내며, T value와 관측치 수에 의해 결정됨
- Tolerance, VIF: 공차한계, 분산팽창지수. 다중공선성 판단 가능. (tolerance)*(VIF)=1 임을 참고

> 변수 선택 알고리즘
- **전진 선택법(Forward Selection)**: 가장 단순한 변수선택법으로, 절편만 있는 모델에서 시작하여 유의미한 독립변수 순으로 **변수를 차례로 하나씩 추가**하는 방법. 새로운 변수를 추가했을 때 모델 적합도가 기준치 이상 증가하지 못했을 때 변수선택 종료. 
- **후진 선택법(Backward Elimination)**: 모든 독립변수가 포함된 상태에서 시작하여 유의미하지 않은 순으로 **하나씩 제거**하는 방법. 어떤 변수를 제거했을 때, 모델 적합도가 기준치 이상 감소하면 더 이상 변수를 제거하지 않고 종료(전진 선택과 정확히 반대). 유의미한 변수를 처음부터 모두 넣고 시작하기에 **전진선택법보다는 안전한 방법**임.
- **단계적 선택법(Stepwise Selection)**: 전진선택법처럼 변수를 추가하기 시작하면서, 선택 변수가 3개 이상이 되면 **변수 추가와 제거를 번갈아가며 수행**함. 단계적 선택법은 단순히 종속변수와의 상관도가 높은 독립변수를 선택하는 것에서 더 나아가, 선택된 독립변수 모델의 잔차를 구하여 선택되지 않은 나머지 변수와 잔차의 상관도를 구하여 변수를 선택한다. 다른 방법보다 **최적의 변수 조합**을 찾아낼 수 있는 가능성이 높지만 그만큼 많은 조합을 비교하기에 **오래 걸린다는 단점**이 있다. 
- 그 외: LARS(Least Angle Regression), 유전 알고리즘(Genetic Algorithm), Riodge와 Lasso를 조합한 Elastic Net

<br/>

#### Ridge와 Lasso 그리고 Elastic Net

- Ridge(L2-norm): 전체 변수를 모두 유지하면서 각 변수의 계수 크기를 조정함. 예측에 영향을 거의 미치지 않는 변수는 0에 가까운 가중치를 주어 독립변수들의 영향력 조정. -> 계수 정규화(Regularization), 다중공선성을 방지하면서 모델 설명력 최대화 가능

- Lasso(L1-norm): Ridge와 유사하지만, 중요한 몇 개의 변수만 선택하고 나머지 변수들은 계수를 0으로 주어 영향력을 아예 없앤다. -> 모델 단순화, 해석 용이

![img10](assets/img/DBRS_week5/img10.png)


- Elastic Net: Ridge와 Lasso의 최적화 지점이 다르기에 두 정규화 항을 결합하여 절충한 모델. Ridge는 변환된 계수가 0이 될 수 없지만 Lasso는 0이 될 수 있다는 특성을 결합한 것. Ridge와 Lasso의 혼합비율(r)을 조절하여 Elastic Net 모델의 성능을 최적화. (r=0: Ridge, r=1: Lasso) 


![img11](assets/img/DBRS_week5/img11.png)

> 다중 회귀분석 모형 결과
1. 모델에 대한 유의도 : p-value<0.05
2. 모델 설명력 : 종속변수를 77.3% 설명 가능 
3. 수정된 모델 설명력 : 무의미한 독립변수의 영향을 상쇄하기 위한 기준값, 모델 설명력이 과도하게 높게 나오지 않도록 함.

![img12](assets/img/DBRS_week5/img12.png)


- 선형 회귀분석과 Elastic Net 실습: 캐글 "House Sales in King County"


<br/>

## 부교재

<br/>

## 10장 인과와 상관


### 10.1 인과와 상관

- 인과관계: 원인과 결과의 관계
- 상관관계: 데이터에서 보이는 관련성

- 중첩: 두 변수에 관련된 외부 변수가 존재하는 경우
- 중첩요인(중첩변수, confounder): 중첩일 때의 그 해당 변수

![img13](assets/img/DBRS_week5/img13.png)

- 허위상관: 인과관계는 없지만 상관관계는 있는 경우

![img14](assets/img/DBRS_week5/img14.png)

> 상관관계가 있다는 사실만으로 인과관계의 유무를 구별할 수 없다!


> 인과관계를 알면 원인 변수를 변화시킴으로써(개입), 결과 변수를 바꿀 수 있다. 

> 상관관계는 2개 변수 사이의 관련성이므로, 한쪽 변수로부터 또 다른 변수를 예측할 수 있다. 


- 시간, 나이는 중첩요인이 되기 쉽다. 즉 X, Y가 직접 연관되지는 않았지만 각각 중첩(시간) 변수와 연관되어있어 두 변수 사이에 상관관계가 나타날 수 있다. 
(ex.초콜릿 소비량과 노벨상 수상자 수) 


- 우연히 생긴 상관: 수많은 변수를 마구잡이로 해석하면 우연하게도 통계적으로 유의미한 결과를 얻을 수도 있다. 

<br/>

### 10.2 무작위 통제 실험

인과관계는 중첩요인이 존재하기에 발견하기 어렵다. 

![img15](assets/img/DBRS_week5/img15.png)

위의 사진에서 마치 음주가 폐암에 영향을 주는 것처럼 보이지만, 이는 음주 이외에 '흡연'이라는 중첩요인으로 인해 발생한 상황이다. 

때문에, 음주가 폐암에 미치는 효과를 알고자 한다면, 음주 이외의 요인을 동이랗게 하지 않으면 안된다. 

>무작위 통제 실험(randomized control trial, RCT, a.k.a. 'AB 테스트')

인과효과를 추정하는 가장 강력한 방법은 무작위 통제 실험(RCT)이다. 알고자 하는 요인인 변수 X에 표본을 무작위로 할당하고 개입 실험을 수행한 다음, 변수 Y와 비교하는 방법이다. 

- 무작위 통제 실험이 인과효과를 추정하는 강력한 방법인 이유: 중첩요인을 확인하지 않더라도, 그 효과를 무작위를 이용하여 무효화 할 수 있으므로, 알고자 하는 변수의 효과만 추정 가능

![img16](assets/img/DBRS_week5/img16.png)

- 인과추론의 근분 문제: 이 사람이 다이어트를 한 세계와, 하지 않은 세계 양쪽을 모두 관찰할 수는 없다. 

- 선택편향: 관측 가능한 τ'은 원래 알고자 하는 효과 τ에 편향이 더해진 값이 됨

> 중첩요인의 존재가 선택편향을 발생시켰고, 이로 인해 인과관계를 밝혀 내기 어려웠다. 


<br/>

### 10.3 통계적 인과 추론

무작위 통제 실험은 인과효과를 추정하는 데 사용하는 강력한 방법이지만 언제나 개입 실험이 가능한 것은 아니다. 

때문에 통계적 인과 추론이라는 인과효과의 추정 방법 및 통계적 인과 탐색이라는 인과구조의 지정방법 개발이 활발하게 진행 중이다. 

- 다중회귀: *y = a + bx + cz* 라는 다중회귀 모형에서  b,c는 인과효과를 나타낸다. 그러기 위해서는 생각할 수 있는 중첩요인을 측정해 모형에 도입하는 것이 중요하다. 이를 **조정**한다고 표현한다. 

- 층별 해석: 중첩요인을 기준으로 데이터를 몇 가지 그룹(층)으로 나누어, 각 층에서 중첩요인의 효과를 가능한 한 작게 하는 방법

![img17](assets/img/DBRS_week5/img17.png)

- 경향 점수 짝짓기(matching): 원인변수=0인 집단과, 원인변수=1인 집단에서 비슷한 중첩요인을 가진 데이터를 골라 쌍으로 만드는 방법. 특히 경향 점수 짝짓기(Propensity Score Matching, PSM)는 경향 점수라는 1차원 값을 기준으로 쌍을 고르는 방법으로 자주 사용된다. 

![img18](assets/img/DBRS_week5/img18.png)

- 이중차분법(Difference In Differences, DID): 서로 다른 두 집단 A, B에 대해 A에는 처리를 가하고 B에는 가하지 않은 연구 설계에서는, 중첩요인에 따라 인과효과의 추정이 어려울 때가 있다. 이럴 때는 시간 축을 도입, 집단 간 차이에 대해 다시 한번 처리 전후의 차분을 취함으로서 인과효과를 추정한다. 

<br/>

## 11장 베이즈 통계

<br/>

### 11.1 베이즈 통게의 사고방식

> 빈도주의 통계 vs. 베이즈 통계

![img19](assets/img/DBRS_week5/img19.png)

- 빈도주의에서의 확률은 무한히 반복 실행한 결과로서의 객관적인 빈도를 나타내는 반면, 베이즈 통계는 확률을 얼마나 '확신하는지'로 해석하는 원리다. 

![img20](assets/img/DBRS_week5/img20.png)


- 통계적 추론(statistical inference): 데이터의 모집단의 실제 분포 *q(x)*를 추론하는 것

데이터를 이용하여 추정한 통계 모형 *p(x)*가 모집단의 실제 분포 *q(x)*와 어느 정도 들어맞는지를 정량화함으로써, 통계 모형의 적합도를 평가할 수 있다. 

- 최대가능도 방법
![img21](assets/img/DBRS_week5/img21.png)
![img22](assets/img/DBRS_week5/img22.png)

- 최대가능도 추정량
![img23](assets/img/DBRS_week5/img23.png)
![img24](assets/img/DBRS_week5/img24.png)


> 베이즈 통계의 사고방식: 베이즈 통계에서는 통게 모형의 파라미터를 확률변수로 취급하여, 그 확률분포를 생각한다. 

사전분포(prior distribution), *p(x|θ)*를 마련해 두어야 이를 이용해 사후분포(posterior distribution), *p(θ|x)*를 구하는 것이 베이즈 통계에서의 추정이다. 

- 베이즈 정리
![img25](assets/img/DBRS_week5/img25.png)

![img26](assets/img/DBRS_week5/img26.png)

- 베이즈 추정의 예측분포
![img27](assets/img/DBRS_week5/img27.png)

- 정보량 기준
    - 쿨백-라이블러 발산(KL divergence): 두 확률밀도함수를 비교, f(x)와 g(x)가 가까울수록 작은 값 가짐.
![img29](assets/img/DBRS_week5/img29.png)

이 식의 값을 작게 한다는 것은,  AIC를 작게 만드는 것과 같다. 

![img28](assets/img/DBRS_week5/img28.png)

AIC와 같은, 모형의 좋고 나쁨을 평가하는 지표를 **정보량 기준**이라 한다.
베이즈 추정의 경우 WAIC(Widely Applicable Information Criteria)를 작게 하는 것과 같다.


> 베이즈 통계의 이점
1. 추정 결과, 통게 모형의 파라미터를 분포로 얻을 수 있다는 점
2. 베이즈 통계에서 이용하는 계산 방법인 MCMC 방법이 난수를 발생시켜 시뮬레이션으로서 사후분포를 따르는 파라미터를 얻기 때문에 복잡한 모형화 가능


<br/>

### 11.2 베이즈 통계 알고리즘

> 베이즈 통계는 사후분포를 사용하는데, 직접 계산하기 어렵기에 MCMC 방법(Markov Chain Monte Carlo method)이라는 계산 알고리즘을 사용한다. 

- MCMC방법 자체는 특정 확률분포를 따르는 난수 발생 알고리즘이다. 

- 몬테카를로 방법: 난수를 여러 개 발생시켜 시뮬레이션해 근사해를 얻는 방법이다. 

- 마르코프 연쇄: 어떤 상태에서 다른 상태로 변화하는 현상을 확률로 표혆나 모형의 일종

- 깁스 표집(Gibbs sampling): 구체적인 계산은 한쪽 변수를 고정한 뒤, 고정하지 않은 변수를 확률적으로 움직이는 작업을 번갈아 반복하는 순서로 이루어진다. 


<br/>

### 11.3 베이즈 통계 사례

> 이표본 평균값 비교

> 푸아송 회귀의 예

> 계층적 베이지안 모형

