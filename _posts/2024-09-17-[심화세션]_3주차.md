---
layout: post
title: "[심화세션]_3주차"
published: true
date: 2024-10-01
math: true
categories: 
tags: KHUDA 심화세션
---


## 주교재 ##

## [5장] 회원 탈퇴를 예측하는 테크닉 10 ##

DT를 통해 탈퇴를 예측하는 흐름을 배워보자. 

![img50](assets/img/DBRS_week3/img50.png)

![img51](assets/img/DBRS_week3/img51.png)


### 41. 데이터를 읽어들이고 이용 데이터를 수정하자 ###

```python
import pandas as pd
customer = pd.read_csv('customer_join.csv')
uselog_months = pd.read_csv('use_log_months.csv')

# 데이터 가공. 그달과 1달 전 이용 이력만으로 데이터 작성. 테크닉 36과 유사
year_months = list(uselog_months["연월"].unique())
uselog = pd.DataFrame()
for i in range(1, len(year_months)):
    tmp = uselog_months.loc[uselog_months["연월"]==year_months[i]]
    tmp.rename(columns={"count":"count_0"}, inplace=True)
    tmp_before = uselog_months.loc[uselog_months["연월"]==year_months[i-1]]
    del tmp_before["연월"]
    tmp_before.rename(columns={"count":"count_1"}, inplace=True)
    tmp = pd.merge(tmp, tmp_before, on="customer_id", how="left")
    uselog = pd.concat([uselog, tmp], ignore_index=True)
```

- loc(): Pandas에서 데이터프레임의 특정행과 열을 선택하거나 필터링


### 42. 탈퇴 전월의 탈퇴 고객 데이터를 작성하자 ###

이 스포츠 센터에서는 월말까지 탈퇴 신청을 해야 다음 달 말에 탈퇴할 수 있기에 탈퇴를 신청한 시점의 데이터를 예측에 사용해야 한다. 

```python
from dateutil.relativedelta import relativedelta
exit_customer = customer.loc[customer["is_deleted"]==1]
exit_customer["exit_date"] = None
exit_customer["end_date"] = pd.to_datetime(exit_customer["end_date"])
for i in range(len(exit_customer)):
    exit_customer["exit_date"].iloc[i] = exit_customer["end_date"].iloc[i] - relativedelta(months=1)
exit_customer["연월"] = pd.to_datetime(exit_customer["exit_date"]).dt.strftime("%Y%m")
uselog["연월"] = uselog["연월"].astype(str)
exit_uselog = pd.merge(uselog, exit_customer, on=["customer_id", "연월"], how="left")
print(len(uselog))
exit_uselog.head()
```

- iloc[]: 행과 열의 위치 기반 인덱싱 지원
- relativedelta(): 날짜 간의 차이를 나타낼 때 사용
- strftime("%Y%m): 날짜를 특정 형식의 문자열로 변환하기 위해 사용하는 함수, string format time의 약자
- astype(str): pandas에서 열의 데이터 타입을 문자열로 변환하는 데 사용

```python
# name칼럼에 NA만 삭제
exit_uselog = exit_uselog.dropna(subset=["name"])
print(len(exit_uselog))
print(len(exit_uselog["customer_id"].unique()))
exit_uselog.head()
```


### 43. 지속 회원의 데이터를 작성하자 ###

```python
# 지속 회원 데이터만 선택
conti_customer = customer.loc[customer["is_deleted"]==0]
# uselog 기준 left join
conti_uselog = pd.merge(uselog, conti_customer, on=["customer_id"], how="left")
print(len(conti_uselog))
# name칼럼이 null인 행 삭제
conti_uselog = conti_uselog.dropna(subset=["name"])
print(len(conti_uselog))
```
- df.dropna(): 특정 칼럼에서 null값이 있는 행 삭제


```python
# conti_uselog 데이터 프레임의 모든 행을 임의로 섞기. frac=1: 전체 데이터의 100%, drop=True: 기존 인덱스를 버리고 새로 인덱스 부여
conti_uselog = conti_uselog.sample(frac=1).reset_index(drop=True)
# customer_id 기준 중복된 행 제거 (첫 번째 행만 유지)
conti_uselog = conti_uselog.drop_duplicates(subset="customer_id")
print(len(conti_uselog))
conti_uselog.head()
```
- df.sample(frac=1).reset_index(drop=True): 전체 데이터 행의 100%를 섞고 새로 인덱스 부여
- df.drop_duplicates(): 중복된 칼럼이 존재하는 행 삭제


```python
# 데이터를 위 아래로 결합. ignore_index=True: 기존 인덱스를 무시하고 새 인덱스 부여
predict_data = pd.concat([conti_uselog, exit_uselog],ignore_index=True)
print(len(predict_data))
predict_data.head()
```


### 44. 예측할 달의 재적기간을 작성하자 ###


```python
# 초기 설정: period라는 새로운 칼럼을 추가하고 0으로 설정
predict_data["period"] = 0
# 날짜 data type을 datetime으로 변경
predict_data["now_date"] = pd.to_datetime(predict_data["연월"], format="%Y%m")
predict_data["start_date"] = pd.to_datetime(predict_data["start_date"])
# 반복문으로 고객의 가입 기간을 계산하고 period 열에 할당
for i in range(len(predict_data)):
    # start_date와 now_date의 차이 계산
    delta = relativedelta(predict_data["now_date"][i], predict_data["start_date"][i])
    predict_data["period"][i] = int(delta.years*12 + delta.months)
predict_data.head()
```


### 45. 결측치를 제거하자 ###

```python
# 결측치 시각화
predict_data.isna().sum()
# count_1, end_date, exit_date에 결측값 존재

# count_1 결측치만 제거
predict_data = predict_data.dropna(subset=["count_1"])
predict_data.isna().sum()
```


### 46. 문자열 변수를 처리할 수 있게 가공하자 ###

```python
target_col = ["campaign_name", "class_name", "gender", "count_1", "routine_flg", "period", "is_deleted"]
predict_data = predict_data[target_col]
predict_data.head()

# 원-핫 인코딩을 통해 범주형 변수들을 새로운 열로 변환
predict_data = pd.get_dummies(predict_data)
predict_data.head()

# 불필요한 더미 변수 삭제(원-한 인코딩을 하면 0,1로 반환되어 완벽히 상관되는 다수의 열이 반환된다. 여기서 다중공선성이 발생할 수 있기에 범주별로 하나의 열 삭제함)
del predict_data["campaign_name_일반"]
del predict_data["class_name_야간"]
del predict_data["gender_M"]
predict_data.head()
```
- 원-핫 인코딩: 범주형 변수 -> 0, 1로 이루어진 이진 열로 변환 (더미 변수)
- 더미 변수: 수치형 변수로 변환한 이진 변수, 0과 1로 이루어짐. 대부분의 ML모델이 범주형 데이터를 처리하지 못하고 다중공선성을 제거하기 위해 더미 변수로 변환하는 전처리과정이 필요하다. 


### 47. 의사결정 트리를 사용해서 탈퇴 예측 모델을 구축하자

```python
from sklearn.tree import DecisionTreeClassifier
import sklearn.model_selection

# 탈퇴한 회원 필터링
exit = predict_data.loc[predict_data["is_deleted"]==1]
# 유지 회원을 탈퇴 회원 수만큼 랜덤 샘플링 -> 탈퇴 고객과 유지 고객의 비율을 1:1로 맞춰 균형 있는 데이터셋 만들기
conti = predict_data.loc[predict_data["is_deleted"]==0].sample(len(exit))

# X는 예측에 사용할 모든 데이터를 포함
X = pd.concat([exit, conti], ignore_index=True)
# y는 X에서 'is_deleted'열을 가져와서 만든 시리즈로, 각 샘플의 레이블(타깃 값)이다. 
y = X["is_deleted"]
del X["is_deleted"]
# 데이터셋 분할 
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)

# 난수 설정, 모델 훈련
model = DecisionTreeClassifier(random_state=0)
model.fit(X_train, y_train)
# 테스트 데이터로 예측
y_test_pred = model.predict(X_test)
print(y_test_pred)

# y_test와 y_pred를 하나의 데이터 프레임으로 만들기
results_test = pd.DataFrame({"y_test":y_test ,"y_pred":y_test_pred })
results_test.head()
```


### 48. 예측 모델을 평가하고 모델을 튜닝해보자

```python
# 모델 성능 직접 계산
correct = len(results_test.loc[results_test["y_test"]==results_test["y_pred"]])
data_count = len(results_test)
score_test = correct / data_count
print(score_test)

# 모델 성능 함수로 계산 
print(model.score(X_test, y_test))
print(model.score(X_train, y_train))

# 새로 데이터 준비
X = pd.concat([exit, conti], ignore_index=True)
y = X["is_deleted"]
del X["is_deleted"]
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y)

# 결정 트리 모델 생성(복잡도 조정)
model = DecisionTreeClassifier(random_state=0, max_depth=5)
model.fit(X_train, y_train)
print(model.score(X_test, y_test))
print(model.score(X_train, y_train))
```


### 49. 모델에 기여하는 변수를 확인하자 ###

```python
# 특성 중요도 출력 
importance = pd.DataFrame({"feature_names":X.columns, "coefficient":model.feature_importances_})
importance
```
- model.feature_importances_: dt모델이 학습된 후 각 특성의 중요도 반환


### 50. 회원 탈퇴를 예측하자 ###

```python
# 고객 변수 지정
count_1 = 3
routing_flg = 1
period = 10
campaign_name = "입회비무료"
class_name = "종일"
gender = "M"

# 범주형 변수를 더미 변수로 인코딩 (원-핫 인코딩)
if campaign_name == "입회비반값할인":
    campaign_name_list = [1, 0]
elif campaign_name == "입회비무료":
    campaign_name_list = [0, 1]
elif campaign_name == "일반":
    campaign_name_list = [0, 0]
if class_name == "종일":
    class_name_list = [1, 0]
elif class_name == "주간":
    class_name_list = [0, 1]
elif class_name == "야간":
    class_name_list = [0, 0]
if gender == "F":
    gender_list = [1]
elif gender == "M":
    gender_list = [0]
input_data = [count_1, routing_flg, period]
input_data.extend(campaign_name_list)
input_data.extend(class_name_list)
input_data.extend(gender_list)

# 고객의 탈퇴 여부 예측(1: 탈퇴, 0: 유지)
print(model.predict([input_data]))
# 고객의 유지/탈퇴할 확률 반환
print(model.predict_proba([input_data]))
```



## [3부] 최적화 문제 ##

## [6장] 물류의 최적경로를 컨설팅하는 테크닉 10 ###

'물류'는 상품의 매출을 좌우하는 생명선이라고 할 수 있다. 
이 장에서는 먼저 '물류'의 기초가 되는 '운송최적화'를 검토하고 기초적인 기술을 배운다. 

BP: 회사 이익 감소로, 물류비용을 줄이고 효율화 생각중. 먼저, 창고-생산 공장 운송 비용 절감 방안 검토

> 데이터 

| no | 파일 이름 | 개요 |
| --- | --- | --- |
| 1 | tbl_factory.csv | 생산 공장 데이터 |
| 2 | tbl_warehouse.csv | 창고 데이터 |
| 3 | rel_cost.csv | 창고와 공장 간의 운송 비용 |
| 4 | tbl_transaction.csv | 2019년의 공장으로의 부품 운송 실적 |


### 51. 물류 데이터를 불러오자 ###



```python
import pandas as pd

# 공장데이터 불러오기
factories = pd.read_csv("tbl_factory.csv", index_col=0)
factories

# 창고데이터 불러오기
warehouses = pd.read_csv("tbl_warehouse.csv", index_col=0)
warehouses

# 비용 테이블
cost = pd.read_csv("rel_cost.csv", index_col=0)
cost.head()

# 운송 실적 테이블
trans = pd.read_csv("tbl_transaction.csv", index_col=0)
trans.head()

# 운송실적 테이블에 각 테이블을 조인 
# 비용 데이터추가. 기준이 되는 칼럼명이 다르기 때문에 각각 left_on, right_on으로 지정
join_data = pd.merge(trans, cost, left_on=["ToFC","FromWH"], right_on=["FCID","WHID"], how="left")
join_data.head()

# 공장정보 추가
join_data = pd.merge(join_data, factories, left_on="ToFC", right_on="FCID", how="left")
join_data.head()

# 창고정보 추가
join_data = pd.merge(join_data, warehouses, left_on="FromWH", right_on="WHID", how="left")
# 컬럼 정리
join_data = join_data[["TransactionDate","Quantity","Cost","ToFC","FCName","FCDemand","FromWH","WHName","WHSupply","WHRegion"]]
join_data.head()

# 북부 데이터 추출
north = join_data.loc[join_data["WHRegion"]=="북부"]
north.head()

# 남부데이터 추출
south = join_data.loc[join_data["WHRegion"]=="남부"]
south.head()
```

> 데이터 병합 method
- **merge()**: SQL의 join과 매우 비슷한 방식을 작동함.
    - pd.merge(df1, df2, on='key_column'): on을 기준으로 조인 수행, 두 df에 모두 있어야 함 (inner join)
    - pd.merge(df1, df2, left_on='df1_key', right_on='df2_key'):
    - pd.merge(df1, df2, how='left', on='key_column'): how의 기본값은 inner이고, left, right, outer도 가능하다. 
    - pd.merge(df1, df2, on='key_column', suffixes=('_left', '_right')): *suffixes*는 만일 두 df에 같은 이름의 열이 있을 때, 이 열들을 구분하기 위해 접미사를 붙인다. 
    - pd.merge(df1, df2, how='outer', on='key_column', indicator=True): *indicater*는 새로운 열(_merge)에 어떤 df에서 온 데이터인지 보여준다
    - pd.merge(df1, df2, on='key_column', validate='one_to_one'): *validate*는 병합 유형을 검증한다. one_to_one, one_to_many 등을 이 매개변수를 통해 확인할 수 있다. 
- **join()**: 
- **concat()**:
- **update()**: 
- **combine_first()**:


### 52. 현재 운송량과 비용을 확인해 보자 ###

```python
# 지사의 비용합계 계산
print("북부지사 총비용: " + str(north["Cost"].sum()) + "만원")
print("남부지사 총비용: " + str(south["Cost"].sum()) + "만원")

# 지사의 총운송개수
print("북부지사의 총부품 운송개수: " + str(north["Quantity"].sum()) + "개")
print("남부지사의 총부품 운송개수: " + str(south["Quantity"].sum()) + "개")

# 부품 1개당 운송비용
tmp = (north["Cost"].sum() / north["Quantity"].sum()) * 10000
print("북부지사의 부품 1개당 운송 비용: " + str(int(tmp)) + "원")
tmp = (south["Cost"].sum() / south["Quantity"].sum()) * 10000
print("남부지사의 부품 1개당 운송 비용: " + str(int(tmp)) + "원")

# 비용을 지사별로 집계
cost_chk = pd.merge(cost, factories, on="FCID", how="left")
# 평균
print("북부지사의 평균 운송 비용：" + str(cost_chk["Cost"].loc[cost_chk["FCRegion"]=="북부"].mean()) + "원")
print("남부지사의 평균 운송 비용：" + str(cost_chk["Cost"].loc[cost_chk["FCRegion"]=="남부"].mean()) + "원")
```


### 53. 네트워크 가시화 ###

최적화 문제를 푸는 라이브러리는 여러 가지 있지만, 단순히 사용 방법을 배우는 것으로는 실제 현장에서 도움이 안된다. 최적화 프로그램이 도출한 계획이 올바른지, 그 계획을 선택할지 여부는 현장 의사결정권자의 이해 여부에 달려있다. 

그래서 최적화 프로그램에 의해 도출된 계획을 가시화하는 프로세스와 몇 가지 조건을 실제로 만족하는지를 확인하는 프로세스가 중요하다. 최적 경로를 가시화하는 방법인 **네트워크 가시화**를 배워보자. 

```python
import networkx as nx
import matplotlib.pyplot as plt

# 무방향 그래프 객체생성
G=nx.Graph()

# 노드 설정
G.add_node("nodeA")
G.add_node("nodeB")
G.add_node("nodeC")

# 엣지 설정: 노드 간의 연결
G.add_edge("nodeA","nodeB")
G.add_edge("nodeA","nodeC")
G.add_edge("nodeB","nodeC")

# 좌표 설정 (딕셔너리 사용)
pos={}
pos["nodeA"]=(0,0)
pos["nodeB"]=(1,1)
pos["nodeC"]=(0,1)

# 그리기
nx.draw(G,pos)

# 표시
plt.show()
```
![img52](assets/img/DBRS_week3/img52.png)


### 54. 네트워크에 노드를 추가해 보자 ###


```python
import networkx as nx
import matplotlib.pyplot as plt

# 그래프 객체 생성．
G=nx.Graph()

# 노드 설정
G.add_node("nodeA")
G.add_node("nodeB")
G.add_node("nodeC")
G.add_node("nodeD")

# 엣지 설정
G.add_edge("nodeA","nodeB")
G.add_edge("nodeA","nodeC")
G.add_edge("nodeB","nodeC")
G.add_edge("nodeA","nodeD")

# 좌표 설정
pos={}
pos["nodeA"]=(0,0)
pos["nodeB"]=(1,1)
pos["nodeC"]=(0,1)
pos["nodeD"]=(1,0)

# 그리기, label 표시
nx.draw(G,pos, with_labels=True)

# 표시
plt.show()
```
![img53](assets/img/DBRS_week3/img53.png)


### 55. 경로에 가중치를 부여하자 ###

가중치를 이용해 엣지 굵기를 바꾸면 물류의 최적 경로를 알기 쉽게 가시화 할 수 있다. 
여기서는 csv파일에 저장된 가중치 정보를 가져와 이용한다. 

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

# 데이터 불러오기
df_w = pd.read_csv('network_weight.csv')
df_p = pd.read_csv('network_pos.csv')

# 그래프 객체 생성
G = nx.Graph()

# 노드 설정
for i in range(len(df_w.columns)):
    G.add_node(df_w.columns[i])

# 엣지 설정 & 가중치 리스트화
size = 10
edge_weights = []
num_pre = 0
# 엣지 가중치 확인용 번역자 추가 코드
name = ['A','B','C','D','E']

for i in range(len(df_w.columns)):
    for j in range(len(df_w.columns)):
        if not (i==j):
            # 엣지 추가
            G.add_edge(df_w.columns[i],df_w.columns[j])
            if num_pre<len(G.edges):
                num_pre = len(G.edges)
                # 엣지 가중치 추가
                edge_weights.append(df_w.iloc[i][j]*size)
               
                # 엣지 가중치 확인용 번역자 추가 코드
                print(f'({name[i]}, {name[j]}) = {np.round(edge_weights[-1],5)}')
               

# 좌표 설정
pos = {}
for i in range(len(df_w.columns)):
    node = df_w.columns[i]
    pos[node] = (df_p[node][0],df_p[node][1])

# 그리기
nx.draw(G, pos, with_labels=True,font_size=16, node_size = 1000, node_color='k', font_color='w', width=edge_weights)

# 표시
plt.show()
```
![img54](assets/img/DBRS_week3/img54.png)


### 56. 운송경로 정보를 불러오자 ###

```python
import pandas as pd

df_tr = pd.read_csv('trans_route.csv', index_col="공장")
df_tr.head()
```


### 57. 운송경로정보로 네트워크를 가시화해보자 ###


```python
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx

df_tr = pd.read_csv('trans_route.csv', index_col="공장")
df_pos = pd.read_csv('trans_route_pos.csv')


# 그래프 객체 생성
G = nx.Graph()

# 노드 설정
for i in range(len(df_pos.columns)):
    G.add_node(df_pos.columns[i])

# 엣지 설정 및 가중치 리스트화
num_pre = 0
edge_weights = []
size = 0.1
for i in range(len(df_pos.columns)):
    for j in range(len(df_pos.columns)):
        if not (i==j):
            # 엣지 추가
            G.add_edge(df_pos.columns[i],df_pos.columns[j])
            # 엣지 가중치 추가
            if num_pre<len(G.edges):
                num_pre = len(G.edges)
                weight = 0
                if (df_pos.columns[i] in df_tr.columns)and(df_pos.columns[j] in df_tr.index):
                    if df_tr[df_pos.columns[i]][df_pos.columns[j]]:
                        weight = df_tr[df_pos.columns[i]][df_pos.columns[j]]*size
                elif(df_pos.columns[j] in df_tr.columns)and(df_pos.columns[i] in df_tr.index):
                    if df_tr[df_pos.columns[j]][df_pos.columns[i]]:
                        weight = df_tr[df_pos.columns[j]][df_pos.columns[i]]*size
                edge_weights.append(weight)
                

# 좌표 설정
pos = {}
for i in range(len(df_pos.columns)):
    node = df_pos.columns[i]
    pos[node] = (df_pos[node][0],df_pos[node][1])
    
# 그리기
nx.draw(G, pos, with_labels=True,font_size=16, node_size = 1000, node_color='k', font_color='w', width=edge_weights)

# 표시
plt.show()
```
![img55](assets/img/DBRS_week3/img55.png)


### 58. 운송비용함수를 작성하자 ###

```python
import pandas as pd

# 데이터 불러오기
df_tr = pd.read_csv('trans_route.csv', index_col="공장")
df_tc = pd.read_csv('trans_cost.csv', index_col="공장")

# 운송 비용 함수
def trans_cost(df_tr,df_tc):
    cost = 0
    for i in range(len(df_tc.index)):
        for j in range(len(df_tr.columns)):
            cost += df_tr.iloc[i][j]*df_tc.iloc[i][j]
    return cost

print("총 운송 비용:"+str(trans_cost(df_tr,df_tc)))
```


### 59. 제약조건을 만들어보자 ###

```python
import pandas as pd

# 데이터 불러오기
df_tr = pd.read_csv('trans_route.csv', index_col="공장")
df_demand = pd.read_csv('demand.csv')
df_supply = pd.read_csv('supply.csv')

# 수요측 제약조건
for i in range(len(df_demand.columns)):
    temp_sum = sum(df_tr[df_demand.columns[i]])
    print(str(df_demand.columns[i])+"으로 운송량:"+str(temp_sum)+" (수요량:"+str(df_demand.iloc[0][i])+")")
    if temp_sum>=df_demand.iloc[0][i]:
        print("수요량을 만족시키고있음")
    else:
        print("수요량을 만족시키지 못하고 있음. 운송경로 재계산 필요")

# 공급측 제약조건
for i in range(len(df_supply.columns)):
    temp_sum = sum(df_tr.loc[df_supply.columns[i]])
    print(str(df_supply.columns[i])+"부터의 운송량:"+str(temp_sum)+" (공급한계:"+str(df_supply.iloc[0][i])+")")
    if temp_sum<=df_supply.iloc[0][i]:
        print("공급한계 범위내")
    else:
        print("공급한계 초과. 운송경로 재계산 필요")
```


### 60. 운송경로를 변경해서, 운송비용함수의 변화를 확인하자 ###


```python
import pandas as pd
import numpy as np

# 데이터 불러오기
df_tr_new = pd.read_csv('trans_route_new.csv', index_col="공장")
print(df_tr_new)

# 총 운송비용 재계산 
print("총 운송 비용(변경 후):"+str(trans_cost(df_tr_new,df_tc)))

# 제약조건 계산함수
# 수요측
def condition_demand(df_tr,df_demand):
    flag = np.zeros(len(df_demand.columns))
    for i in range(len(df_demand.columns)):
        temp_sum = sum(df_tr[df_demand.columns[i]])
        if (temp_sum>=df_demand.iloc[0][i]):
            flag[i] = 1
    return flag
            
# 공급측
def condition_supply(df_tr,df_supply):
    flag = np.zeros(len(df_supply.columns))
    for i in range(len(df_supply.columns)):
        temp_sum = sum(df_tr.loc[df_supply.columns[i]])
        if temp_sum<=df_supply.iloc[0][i]:
            flag[i] = 1
    return flag

print("수요조건 계산결과:"+str(condition_demand(df_tr_new,df_demand)))
print("공급조건 계산결과:"+str(condition_supply(df_tr_new,df_supply)))
```





## 부교재 ##

## [6장] 다양한 가설검정 ##

### 6.1 다양한 가설검정 ###

![img1](assets/img/DBRS_week3/img1.png)

> 가설검정 해석의 기본 흐름
![img2](assets/img/DBRS_week3/img2.png)


- 분할표: 각 상태에 몇 개 데이터가 있는지를 나타내는 표
- 산점도: x-y 평면에 데이터를 점으로 찍어 양적 변수간의 관계를 표현한 그림

![img3](assets/img/DBRS_week3/img3.png)


> 데이터 유형 먼저 확인

- 데이터 유형이 양적 변수인지 질적 변수인지에 따라 해석 방법이 달라진다.
- 표본의 수(집단의 수)도 분석 방법 선택에서 중요한 요소다. 

![img4](assets/img/DBRS_week3/img4.png)


> 양적 변수의 성질

- 모수검정: 모집단이 수학적으로 다룰 수 있는 특정 분포를 따른다는 가정을 둔 가설검정
- 비모수검정: 모집단분포가 특정 분포라고 가정할 수 없는 경우, 예를 들어 좌우 비대칭 분포나 이상값이 있는 분포 -> (예시 이미지)

- 정규성: 데이터가 정규분포로부터 얻어졌다고 간주할 수 있는 성질

![img5](assets/img/DBRS_week3/img5.png)

- 등분산성: 분산이 같은 성질, 집단 간 평균값을 비교하는 경우에 집단끼리 분산이 동일하다고 가정하는 경우가 많음
![img6](assets/img/DBRS_week3/img6.png)


### 6.2 대푯값 비교 ###

- 일표본 t검정: 2가지 조건을 비교x, 어떤 평균값의 모집단에서 표본을 얻었는가를 조사하는 것

> 가설 예시
- 귀무가설: 모집단의 평균은 m=00 이다.
- 대립가설: 모집단의 평균은 m=00이 아니다. 

여기서 00은 연구의 배경으로서 의미가 있는 값을 고려해야만 한다. 

예를 들어, '한국인 성인 남성의 평균 키는 170cm이다'와 같은 맥락이 없는 가설은 가설검정보다 신뢰구간을 구하는 방법이 적절하다.

![img7](assets/img/DBRS_week3/img7.png)

95% 신로ㅓㅣ구간을 구하는 것과 @=0/05의 유의수준으로 귀무가설을 검정하는 것은 x_bar에서 생각할지 귀무가설 m=00에서 생각할지의 차이뿐으로, 동전의 양면과 같은 관계다. 


- 이표본 t검정

이표본 t검정에서는 2개 집단의 평균값을 비교한다. 

![img8](assets/img/DBRS_week3/img8.png)

t검정은 모수검정으로 분류되는 검정 방법이기에 데이터에 정규성이 있어야 한다. 
일반적인 t검정에서는 등분산성을 가정한다. 

만약, 분산이 일치하지 않는 경우에는 **웰치의 t검정**을 이용한다.

**웰치의 t검정**은 2개 집단의 분산이 다르더라도 사용 가능하다. 단, 정규성은 보장되어야 한다.


- 대응 관계가 없는 검정과 대응 관계가 있는 검정
![img9](assets/img/DBRS_week3/img19.png)

대응 관계가 있는 경우는 한 피험자의 혈압을, 약을 복용하기 전후로 총 두 차례 검사한다. 

*대응 관계가 있는 데이터*일 때는 똑같이 *대응 관계가 있는 검정*을 이용하는 편이 좋다. 
그러면 제2종 오류가 발생할 확률이 낮아지며, 검증력도 오르는 경향이 있다. 

> 적절치 않은 검정을 사용한다면?:
- 정규분포에서 데이터를 얻었다고 볼 수 없을 때 t검정을 사용하면, 유의수준 @를 0.05로 설정하더라도 제1종 오류가 일어날 확률이 0.05가 아니게 된다. 이는 가설검정에서 큰 문제가 된다. 
- 한편, p값이 (그 정의인) 귀무가설 하에서 관찰된 값 이상으로 극단적인 값을 얻을 수 있는 확률보다 커지게 되면, 제1종 오류를 일으킬 확률이 설정한 값보다 작아진다. 그만큼 제2종 오류가 일어나기 쉬우며, '보수적인 방법'이라 표현되기도 하지만 이 역시 문제다. 
- 때문에 데이터 성질에 맞춰 적절한 검정 방법을 선택하는 것이 중요하다.


- 정규성 조사

정규성 조사법: Q-Q 플롯, 샤피로-윌크 검정, 콜모고로프-스미르노프 검정 등

![img10](assets/img/DBRS_week3/img10.png)

위의 전제를 잉요해 가설검정을 시행하자. 
p>=0.05라면, 정규성이 있고
p<0.05라면, 정규성이 없다고 판단하는 경우가 많다. 
단,p>0.05라 귀무가설을 기각할 수 없다고 해도 귀무가설이 옳다는 증거가 되지는 않기에, 정규분포에서 데이터를 얻었다고 적극적으로 주장할 수 없다는 점에 유의해야 한다.

또한, 2개 집단이 있을 때는 먼저 각 집단의 정규성을 조사하는 검정을 실시하고 t검정을 이용하게 되므로 가설검정 작업을 반복해 버리는 *검정 다중성 문제*가 생긴다. 
때문에 데이터가 정규분포에서 얻어졌는지 여부를 조사하는 가장 좋은 특정 방법은 없다고 할 수 있다. 
더군다나 현실의 데이터 대부분은 엄밀한 정규분포로부터 나온 것이 아니기에 표본크기가 큰 경우에는 조금만 정규분포에서 벗어나도 정규분포에서 얻은 것이 아니라 판단되고 만다. 


- 등분산성 조사

분산이 같다는 가설을 조사하는 검정으로 **바틀렛 검정**이나 **레빈 검정**이 있다.

![img11](assets/img/DBRS_week3/img11.png)

이때 위의 전제를 이용하여 가설검정을 시행한다. 
p>=0.05라면 등분산, p<0.05라면 부등분산이라 판단하지만, 
정규성 검정과 마찬가지로 p>0.05라 해도 적극적으로 분산이 같다고 주장할 수는 없다. 

> 비모수검정의 대푯값 비교

각 집단 데이터에 정규성이 없는 경우에는 비모수검정으로 분류되는 방법을 사용하는 것이 권장된다. 
이때는 평균값 대신 분포의 위치를 나타내는 대푯값에 주목하여 해석해야 하는데, 대표적인 방법으로는 **윌콕슨 순위합 검정**이 있다.
이는 평균값 대신 데이터 값의 순위에 기반하여 검정을 실시한다. 

**맨-휘트니 U 검정**도 같은 방법이다. 이 방법을 사용하려면 비교할 2개 집단의 분포 모양 자체가 같아야 한다. 즉, 분포는 정규분포가 아니더라도 괜찮지만, 분산이 다르다면 문제가 생길 수 있다. 
![img12](assets/img/DBRS_week3/img12.png)

분석 결과 p>=0.05라면 '2개 모집단의 위치가 다르다 할 수 없다'로,
p<0.05라면 '2개 모집단의 위치가 다르다'로 판단한다. 

그 밖에 2개의 모집단을 비교하는 방법으로는 **플리그너-폴리셀로 검정**과 **브루너-문첼 검정**이 있다. 
이 방법은 2개 모집단의 분포 형태가 같지 않을 때도 사용할 수 있는 비모수검정 방법이다. 
(단, 극단적으로 분포 형태가 다른경우는 제1종 오류가 발생할 확률이 설정한 @와는 달라지므로 주의해야 한다.)

 
> 분산분석(3개 집단 이상의 평균값 비교)

3개 이상 집단의 평균값을 비교하는 방법이 **분산분석(ANOVA, Analysis of variance)**이다. 
예를 들어, 비료 A, B, C에 따라 식물 줄기의 길이에 차이가 생기는지를 조사할 때는 분산분석을 활용하게 된다. 

![img13](assets/img/DBRS_week3/img13.png)

![img14](assets/img/DBRS_week3/img14.png)

- 분산분석의 원리

먼저 집단의 차이를 무시하고 모든 데이터를 이용해 전체 평균을 계산하자. 
여기서 x_bar = 35.2이다. 다음으로, A,B,C 각 집단의 평균을 계산한다. 그리고 각 데이터 x_i와 x_bar의 차이를 구한다. 
예를 들어, x_i가 C군 데이터일 때, 

![img15](assets/img/DBRS_week3/img15.png)
이처럼 계산하여 각 데이터의 전체 평균과의 차이를 집단 내의 분산과 집단 간의 분산으로 분해할 수 있다. 

![img16](assets/img/DBRS_week3/img16.png)

집단 내 변동은 동일 조건에서의 데이터 퍼짐이므로 원래 존재하는 무작위 오차의 크기를 나타낸다. 
이와 달리 집단 간 변동은 집단 간의 차이를 나타내며, 집단 간 차이가 있다면 큰 데이터 퍼짐을, 반대로 차이가 없다면 집단 내 편차와 같은 정도로 작은 데이터 퍼짐을 기대할 수 있다. 

구체적으로 F값 = (평균적인 집단 **간** 변동)/(평균적인 집단 **내** 변동)을 계산하여 검정통계량을 만든다. 이 양은 귀무가설이 올바르다는 가정하에, F분포를 따른다. 
이 분포에서 관찰한 F값 이사응로 극단적인 값이 나올 확률이 p값이다. F값이 유의수준보다 오른쪽에 있다면 유의수준 @=0.05에서 통계저긍로 유의미한 집단 간 차이가 있다는 것이다.

![img17](assets/img/DBRS_week3/img17.png)

> 자유도: 자유로이 움직일 수 있는 변수의 수. 예를 들어 n=10인 표본에서 표본평균을 계산한 이후에의 자유도는 9

> 다중비교 검정
- 그림 6.2.4의 데이터 예시를 분산분석으로 해석하면 F값은 60.7의 큰 값이며, p값은 10^(-7)보다 작다. 따라서 통계적으로 유의미한 집단 간의 차이가 있으므로, 비료 효과에 차이가 난다는 사실을 알 수 있다. 
- 단, 분산분석의 대립가설은 '적어도 한 쌍에는 차이가 있다'이기에 p<0.05로 대립가설을 채택하더라도 어느 쌍에 차이가 있는지까지는 알 수 없다. 그러므로 어느 쌍에 차이가 있는지 알고 싶다면 다중비교라 불리는 방법을 사용해서 조사해야 한다. 
- 집단이 셋 이상일 때 각 쌍의 차이를 조사하기 위해 유의수준 @=0.05에서 이표본 t검정을 반복해 실행하면, 제1종 오류가 증가하고 마는 문제가 생긴다. 예를 들어 3개 집단이 있을 때, 쌍의 수는 3이다. t검정을 3번 반복하면 적어도 적어도 한 쌍에서 제1종 오류가 일어날 확률은 1-(1-0.95)^3=0.143이 되어, 분산분석 전체에서 설정한 유의수준 @=0.05를 상회해버린다.**집단이 늘어날 수록 쌍은 더 빠르게 증가해 제1종 오류가 일어나기 쉬워진다.*
- 즉 몇 번씩 검정을 반복하는 것을 통해, 실은 차이가 없는데도 차이가 있다고 말하는 잘못이 간단히 일어나게 된다. 이 다중성 문제를 회피하고자 **다중비교 검정**을 이용한다. 다중 비교의 기본 아이디어는 검정을 반복하는 만큼, 유의수준을 엄격한 값으로 변경하는 것이다. 

- 여러가지 다중비교 방법

- **본페로니 교정**: 전체에서 유의수준 @를 설정했을 때의 검정 반복 횟수를 k라 하고, 매 검정에서는 @를 검정 횟수로 나눈값을 기준으로 가설검정을 하는 방법. 무척 간편하여 평균값 비교뿐 아니라 다양한 다중비교에서 사용할 수 있지만 검정력이 낮은 경향이 있어, 정말로 차이가 있을 때에 차이가 있다고 주장하기 어렵다. 

- **튜키검정**: 본페로니 교정에서 검정력을 개선한 방법
![img18](assets/img/DBRS_week3/img18.png)

- **던넷 검정**: 모든 쌍을 비교하는 튜키 검정보다도 검정력 향상

- **윌리엄스 검정**: 집단 간에 순위를 매길 수 있는 경우에 사용하면 검정력이 좋다. 

![img19](assets/img/DBRS_week3/img19.png)

- 언제나 분산분석이 필요할까?
다중비교 검정은 최초에 분산 분석을 실행하고, 도출한 p값이 @보다 작을 때 사용하는 것이 일반적이다. 

- 분산분석과 동일한 원리의 다중비교: 분산분석에서의 '통계적으로 유의미하다/유의미하지 않다'의 결과가 다중비교 결과와 일치한다.
- 분산분석과 다른 원리의 다중비교: 분산분석에서의 결과와 다중비교 결과가 다를 때가 있다. 

이러한 경우 분산분석을 실행하지 않고, 다중비교만 실행하는 순서를 따라도 문제없다. 섣불리 분산분석 -> 다중비교를 실행해 버리면, 앞서 본 검정의 다중성 문제가 불거지게 된다. 

본페르니 검정, 튜키 검정, 던넷 검정, 윌리엄스 검정은 분산분석과는 다른 원리이므로 분산분석 없이 단독으로 수행해도 문제 없다. 

- 3집단 이상의 비모수 검정

분산 분석은 모수검정으로 분류된다. 따라서 각 집단의 데이터에는 정규성이 있어야 한다. 
정규성이 없는 집단이 있다면 크러스컬-월리스 검정을 사용하는 것이 좋다. 
비모수 다중비교 방법도 고안되어 있는데, 튜키 검정에 상응하는 것이 **스틸-드와스 검정**, 던넷 검정에 상응하는 것이 **스틸 검정**이다. 


### 6.3 비율 비교 ###

양적 변수에서 모집단의 평균값을 추정하거나 모집단을 대상으로 가설을 세워 가설검정을 시행한 것처럼 범주형 변수에서도 확률 p를 추정하거나 p에 관련된 가설을 세워 검정할 수 있다. 

> 이항검정

하나의 범주가 확률 P, 또 하나의 범주가 확률 1-P로 나타나는지를 조사하는 **이항검정**이라는 방법을 사용한다. 아래와 같이 p=1/2로 귀무가설과 대립가설을 세우고 치우치지 않은 동전인지 알아보자. 

![img20](assets/img/DBRS_week3/img20.png)

귀무가설이 옳다고 가정하고 앞면이 21번, 뒷면이 9번  이상으로 극단적인 값이 나올 확률인 p값을 계산하자. 확률 P로 앞면이, 1-P로 뒷면이 나온다면 N번 동전을 던졌을 때 m번 앞면이 나올 확률은 이항분포를 따른다. 여기서는 p=0.043이므로 유의수준 @=0.05에서 통계적으로 유의미하게 한쪽으로 치우쳤다고 판단할 수 있다. 즉, 가설검정의 관점에서는 편향된 동전이라 말할 수 있는 것이다. 

![img21](assets/img/DBRS_week3/img21.png)


> 카이제곱검정: 적합도검정

이항검정은 범주가 2개일 때만 이용할 수 있다. 범주가 더 많을 때나 일반적인 이산확률분포에 이항검정의 방식을 적용하고 싶다면 **카이제곱검정**의 일종인 **적합도검정**을 이용함으로써, 특정 이산확률분포에서 얻은 데이터인지를 조사할 수 있다. 

예를 들어 정상 주사위라면 1/6의 확률로 각 눈이 나오는 이산균등분포를 보인다. 

일반적으로 카이제곱검정의 적합도검정은 다음과 같이 해석한다. 
![img22](assets/img/DBRS_week3/img22.png)

1. 카이제곱검정의 적합도검정에서는 먼저 귀무가설의 확률분포에서 얻을 수 있는 기대도수(전체 개수 * 각 확률)를 계산한다. 
2. 각 눈의 (실제 출현도수-기대도수)^2/(기대도수)를 계산하고, 이를 더한 값을 구한다. 이 검정통계량을 카이제곱값이라 부르는데, 귀무가설이 옳다면 이는 카이제곱분포라는 확률분포를 따른다. 이 분포 안에서 실제로 얻은 카이제곱 값의 위치를 구하여 p 값을 도출한다. 

예시 데이터에서는 p=0.017이 되어 대립가설을 채택하므로, 통계적으로 유의미하게 이 주사위는 올바른 주사위가 아니라고 판단할 수 있다. 

![img23](assets/img/DBRS_week3/img23.png)

각 눈이 1/6씩 나온다는 균등한 확률 분포 말고도 임의의 확률분포도 사용할 수 있다. 
예를 들어 한국인의 혈액형 비율이 A:B:O:AB=34:27:28:11 이라고 할 때, 일본 역시 이 비율을 따르는지 조사하려면 각 혈액형 확률을 0.34, 0.27, 0.28, 0.11로 한 확률분포를 이용하면 된다. 

> 카이제곱검정: 독립성검정

이항검정이나 카이제곱검정의 적합도 검정은 '데이터 vs 모집단'의 확률 분포를 비교한 것이다.

그런데 범주형 변수에서도 2개 변수의 관계를 조사해야 할 때가 있다. 

상수리나무와 굴밤나무를 관찰하여 사슴벌레의 개체 수를 암수별로 관찰한다고 하자. 여기서 문제는 나무 종류에 따라 암수의 비율이 달라지는지 여부다. 
2개의 범주형 변수 데이터는 분할표로 정리할 수 있는데, 분할표 가로 방향과 세로 방향의 2개 변수에 주목하자면, 한쪽 변수의 범주가 바뀌었을 때 다른 쪽 변수의 범주 비율이 달라지지 않을 때, 2개 변수는 독립적이라고 말할 수 있다. 

이를 조사하려면 독립성검정을 이용한다.
여기서도 카이제곱값이 등장하여 카이제곱분포를 이용하므로, 카이제곱검정의 독립성검정이라 부른다. 이 검정에서는 아래와 같이 해석한다. 
![img24](assets/img/DBRS_week3/img24.png)

구체적인 계산은 (실제 출현도수-기대도수)^2/(기대도수)를 이용하므로, 적합도 검정과 많이 닮아있다. 기대도수는 분할표의 각 행과 열의 합을 계산한 뒤, 열의 합 비율을 기준으로 다시 배분하여 얻을 수 있다. 

![img25](assets/img/DBRS_week3/img25.png)

기대도수를 얻었다면 계산식에 따라 카이제곱값을 계산한다. 이 예에서 카이제곱값은 4.33인데, 분포 안의 위치를 구하면 p값=0.037이 되므로 대립가설이 채택된다. 즉, 사슴벌레 암수의 비율이 나무 종류에 따라 다르다는 결론을 내릴 수 있다. 

그 밖의 독립성검정으로는 피셔의 정확검정이 있다. 이는 이항검정에서 본 것처럼 초기하분포를 이용하여 모든 경우의 확률을 계산하는 방법이다. 



## [7장] 상관과 회귀 ##


### 7.1 양적 변수 사이의 관계를 밝히다 ###

양적변수 사이의 관계를 분석하는 또 다른 방법으로 상관과 회귀가 있다. 

2개의 양적 변수로 이루어진 데이터를 생각해보자. 여기서 각 대상(학생)으로부터 수학과 과학 양쪽의 데이터를 얻어 쌍을 만들었다는 점이 중요하다. 

> 산점도

1개의 변수를 x축 값으로, 다른 1개의 변수를 y축 값으로 하여 2차원 평면 위에 점으로 나타낼 수 있다. 이렇게 2차원 평면 위에 점으로 대상을 나타낸 그래프를 산점도라 한다. 

![img26](assets/img/DBRS_week3/img26.png)


> 상관

산점도를 이용하여 시각화하면 두 변수가 어떤 관계에 있는지 대략적으로 파악할 수 있다. 

![img27](assets/img/DBRS_week3/img27.png)

두 변수 사이의 관계성을 상관이라 한다. 이는 양적 변수에 한정되지 않는 개념으로, 2개의 확률변수 또는 데이터 사이의 관계성을 의미한다. *다만, 상관이 있다고 해서 원인과 결과를 뜻하는 인과관계가 있는지까지는 알 수 없다.*

> 회귀

y=f(x)라는 함수를 통해 변수 사이의 관계를 공식화하는 것을 회귀라 한다. 

여기서 x를 설명변수 또는 독립변수, y를 반응변수 또는 종속변수라 한다.

상관과 다르게, 회귀에는 'x에서 y'라는 방향성이 있는데 이는 통상 회귀에서 y가 확률변수이기 때문이다. 다르게 보면, f(x)는 확률분포의 파라미터, 특히 평균을 결정하는 역할을 하는 고정부분인 것이다. 

회귀분석에서는 얻은 데이터에 잘 들어맞는 f(x)를 추정하고, 두 변수 간 관계를 구한다. 


### 7.2 상관관계 ###

양적 변수가 2개 있을 때 관계성이 어느 정도로 강한지를 수치로 나타낼 수 있다면, 대상을 이해하는 데 도움이 된다. 

> 피어슨 상관계수 

피어슨 상관계수 r이라 불리는 값은 2개 양적 변수 사이의 선형관계가 얼마나 직선 관계에 가까운가를 평가한다. 보통 '상관계수'라 하면 피어슨 상관계수를 말할 때가 흔하다. 이때 r의 범위는 -1<=r<=1이다. 

![img28](assets/img/DBRS_week3/img28.png)

이 식의 분자는 공분산이라 부르는 값이다. 분모는 x와 y 각각의 표준편차로 r을 -1부터 +1 범위에 머무르게 만든다. 

![img29](assets/img/DBRS_week3/img29.png)

x가 커질수록 y도 함께 커지고, x가 작아질수록 y도 함께 작아지는 관계성을 **양의 상관**이라 하고, x가 커질수록 y는 작아지고 x가 작아질수록 y는 커지는 관계성을 **음의 상관**이라 한다. 

|r|값에 따라 상관의 강도를 다음과 같이 해석하곤 한다. 
![img30](assets/img/DBRS_week3/img30.png)


- 상관계수 r은 선형관계를 나타낸다

상관계수는 두 양적 변수 사이의 관계성 강도를 정량화하는 데 무척 편리한 방법이나, 몇 가지 주의할 점이 있다. 

1. 피어슨 상관계수 r은 2개 양적 변수의 '선형' 관계성 강도를 정량화한 것이다. 비선형관계는 피어슨 상관계수 r로는 적절하게 정량화할 수 없다.
2. 피어슨 상관계수 r은 선형 관계성의 '강도'를 정량화하기에 직선의 기울기크기는 관계가 없다. 즉, 기울기가 크든 작든 r에는 영향을 미치지 않는다. 
![img31](assets/img/DBRS_week3/img31.png)

- 상관계수가 같은 다양한 데이터
같은 r값을 가지고 있더라도, 비선형을 포함해 다양한 패턴이 있을 수 있다. 때문에 상관계수를 계산하기 전에 산점도를 그려, 데이터가 어떻게 분포하고 있는지를 미리 확인해야 한다. 
![img32](assets/img/DBRS_week3/img32.png)

- 정규성 검사 

피어슨 상관계수 r은 평균이나 분산에 기반한 모수적인 방법이므로, x의 분포, y의 분포가 모두 정규분포라고 가정한다. 

따라서 데이터가 찌그러지거나, 쌍봉형이거나, 이상값이 있을 때에는 적절하지 않다. 

![img33](assets/img/DBRS_week3/img33.png)

실전에서는 상관계수를 계산하기 전에 x축 데이터와 y축 데이터 각각에 대해 정규성을 확인한 후, 한쪽에 조금이라도 정규성이 없다면 비모수 상관계수를 이용하는 것이 좋다. 

> 비모수 상관계수

데이터의 x축, y축 중 적어도 하나 이상에 정규성이 없을 때는, 비모수 상관계수인 **스피어만 순위상관계수** 사용이 권장된다. 

정의상 스피어만 순위상관계수는 양적 데이터의 값 자체가 아니라, 그 데이터 값을 x축, y축 각각에서 크기 순으로 나열했을 때의 1위, 2위, ... 등의 순위로 변환한 다음, 식 **7.1**을 이용하여 계산한다. 

유사하게, **켄달 순위상관계수**가 있다. 스피어만 순위상관계수와 사용 대상은 거의 비슷하나, 표본크기 n이 매우 작을 때(<10)는 켄달 순위상관계수쪽이 유의성검정의 관점에서 더 좋다. 

- 상관관계 사용 시 주의할 점

상관관계를 계산할 때 2개 변수가 처음부터 종속 관계일 때는 주의가 필요하다. 

예를 들어 수학과 과학 점수라는 2개 변수 X, Y가 있을 때,  수학과 과학 점수 합계 X+Y라는 새로운 변수를 만들어 x축에 수학 점수 X, y축에 합계 점수 X+Y라는 새로운 변수를 만들어 x축에 수학 점수 X, y축에 합계 점수 X+Y를 둔 경우, x축의 값이 y축에 포함된다. 그러므로 설령 수학과 과학 점수가 무상관이라 해도 상관이 나타나게 된다. 
+뿐만 아니라 알아차리기 어려운 나누기도 마찬가지!

x축과 y축의 값이 개별 변수일 것, 그리고 나눗셈 등으로 변환하지 않았을 것을 사전에 확인하자. 

![img34](assets/img/DBRS_week3/img34.png)

> 상관계수와 가설

- 상관계수의 가설검정

표본평균 x_bar와 모평균 m에서와 마찬가지로, 표본으로 계산한 상관계수 r의 배경으로는 상관계수 r_p를 갖는 2개 확률변수로 이루어진 모집단분포를 생각할 수 있다. 
그리고 상관계수 r은 모집단분포에서 무작위추출로 얻은 표본에서 계산한 값으로, r_p의 추정값이 된다. 

![img35](assets/img/DBRS_week3/img35.png)

![img36](assets/img/DBRS_week3/img36.png)

위의 그림을 보면, 표본의 상관계수 r은 모집단의 상관인 0을 중심으로 분포하며, 대부분은 절댓값이 작지만 드물게 r=0.47이나 r=-0.44와 같은, 0에서 멀리 떨어진 값이 나타나기도 한다. 
그러므로 n=20으로 얻은 표본의 상관계수가 0.3 정도인 경우는 r_p=0 무상관인 모집단에서 얻은 표본에서도 발생 가능하다고 말할 수 있다. 

> 상관계수의 유의성검정
- 귀무가설: r_p=0
- 대립가설: r_p!=0

이때 t분포를 이용하여 p값을 계산하는데, 이는 위의 그림에서 본 것처럼 귀무가설이 옳을 때 상관계수 r이 나타내는 분포 중, 표본에서 얻은 r이 어디 위치하는지 구하는 것과 같다. 

가령 표본의 상관계수로 r=0.5를 얻었다 하더라도, 모집단의 상관계수 r_p=0에서도 0.5 정도의 상관계수가 나타나는 것이라면, 귀무가설은 기각할 수 없다. 
가설검정을 시행한 결과 p<0.05임을 알고 나서야 비로소 양의 상관이 있다고 주장 가능하다.
또 거꾸로 r=0.5라 해도 p>0.05라면, 통계적으로 유의미한 상관이 있다고 말할 수 없다. 

- 표본크기와 가설검정
n이 매우 클 때는 가설검정의 결과 해석에 주의해야 한다. 가설검정에서는 n이 클수록 모집단이 귀무가설에서 아주 조금만 어긋나더라도 p값이 작아져 p<0.05가 되므로, 통계적으로 유의미하다고 판단된다. 

![img37](assets/img/DBRS_week3/img37.png)

통계적으로 유의미하게 r=0이 아니라고 주장할 수 있으나, 상관계수 자체는 무척 작은 값이므로 2개 변수 사이의 관계성은 아주 약하다는 것을 알 수 있다. 그러므로 *p<0/05라고 해서 곧바로 상관이 있다고 판단하는 것이 아니라, r값 자체에 눈을 돌려 그 크기를 해석할 필요가 있다.*
또한 가설검정과 마찬가지로 모집단의 상관계수 r_p 의 95% 신뢰구간도 계산 가능하다. n이 클수록, 더 좁은 폭의 신뢰구간을 얻을 수 있기에 확실한 추정이 가능해진다. 

> 비선형상관

지금까지 피어슨 상관계수와 스피어만 순위상관계수로 선형관계를 파악했다. 
그러나 이 값들로는 비선형관계를 다룰 수 없다. 
최근 포괄적인 상관의 원리로서 정보량에 기반을 둔 지표가 몇 가지 제안되고 있다. 
이는 'X가 Y에 관해, 또는 Y가 X에 관해 어느 정도의 정보를 포함하는지'의 관점에서 관계성 강도를 정량화하는 것이다. 


### 7.3 선형회귀 ###

회귀란 설명변수 x와 반응변수 y 사이에 y=f(x)라는 함수를 적용시키는 것을 말한다. 

y=a+bx인 가장 단순한 예를 생각해보자. 회귀식 f(x)의 형태를 결정하는 파라미터 a,b를 회귀계수라 한다. 

특정 평가기준에 따라 회귀의 '좋음(적합도)'을 평가하고, 이 회귀계수의 값을 구체적으로 구하는 것이 회귀분석의 큰 흐름이다. 

회귀에서도 모집4단과 실제 표본의 관계를 생각할 수 있는데 이때 모집단은 (입실론)을 학률오차로 한 다음 식과 같은 확률 모형이라 가정한다. 이 모형을 **회귀 모형**이라 한다. 

```
y=a+bx+(입실론)
```

> 회귀 분석을 실행할 때 중요한 점
1. 어떤 회귀식을 적용할 것인가?
2. 어떻게 회귀식을 데이터에 적용할 것인가?
3. 얻은 회귀모형을 어떻게 평가할 것인가?


> 선형회귀: 회귀분석에서 사용하는 회귀식이 '파라미터에 관한' 1차식이 될 때, 이를 **선형회귀**라 부른다. y=a+bx+cx^2+e은 x에 관한 2차식이지만, 파라미터에 관해서는 선형이므로 선형회귀로 분류한다. y=a+bx+e를 회귀식으로 가장 많이 사용하는데, 그 이유로는 1. 실제현상에서 가장 많이 보이는 관계를 표현하는 식이고, 2. 회귀계수 해석이 용이하기 때문이다. 

- 최소제곱법
데이터에 가능한 한 들어맞는 회귀모형이 좋은 모형이라고 생각하자. 다시 말해, 데이터와 회귀식의 차이가 가능한 작다는 것이다. 

회귀식에서 얻은 값과 각 데이터의 차이(잔차)를 제곱하여 모두 더해 E를 구하고, 이 값을 '데이터와 회귀식의 차이'로 본다. 이 E가 클수록 데이터와 회귀식이 크게 어긋나고, 작을수록 회귀식이 데이터에 잘 맞게 된다. E는 회귀계수인 a나 b가 변할 때마다 달라지는데, 이것을 a와 b의 함수로 볼 수 있으므로 E(a,b)로 나타내자. 그리고 E를 최소화하는 a, b를 구하자. 

선형회귀에서 E는 a와 b의 2차 함수이므로 아래로 볼록한 형태가 된다. 즉, E의 최솟값은 볼록한 형태의 바닥이다. 이처럼 데이터와 모형 차이의 제곱을 모두 더한 값 E를 최소화하는 방법을 **최소제곱법**이라 한다. 

![img38](assets/img/DBRS_week3/img38.png)


> 회귀계수

확률오차 epsilon은 x와는 관계가 없고 평균 0, 분산 시그마^2인 어떤 확률분포를 따르는 확률변수라고 가정한다. 이때 최소제곱법으로 얻은 선형회귀 파라미터는 모집단 파라미터 a와 b의 비편향추정량이 된다. 더불어 분산이 일정하다는 가정에서 최소제곱법으로 얻은 추정량은 비편향추정량 중에서도 가장 정밀도가 높다. 이를 **최량선형비편향추정량**이라 하며, 가우스-마르코프 정리가 이를 증명한다. 


- 회귀계수의 가설검정
회귀계수를 대상으로 가설검정을 시행할 수 있다. 이를 위해선 오차 epsilon의 분포가 정규분포라고 추가로 가정해야 한다. 
단, n이 충분히 클 때는 오차항이 정규분포를 따르지 않아도 가설검정을 시행할 수 있다. 

여기서 b에 관심이 있기에 귀무가설을 '기울기 b=0', 대립가설을 '기울기 b!=0'으로 하여 가설검정을 실행한다. 귀무가설이 옳다면 y=a+epsilon이라는 모형이 될 것이다. 

가설검정 결과 p<0.05를 얻었다면 통계적으로 b가 0이 아니라고 주장할 수 있으며, 따라서 x에 대해 기울기 b인 선형관계라는 것을 알 수 있다. 단, 이것만으론 x와 y 사이에 인과관계가 있다고 할 수 없다. 

- 95% 신뢰구간 
모집단의 회귀계수를 추정하면 신뢰구간을 얻을 수 있다. 이는 동일한 방법으로 표본추출과 회귀분석을 100번 실행했더니, 100번 중 95번 정도는 이 범위에 모집단의 모형이 포함되었다는 것을 의미한다. 

- 95% 예측구간
추정한 회귀모형을 기반으로 데이터 그 자체가 분포하는 구간을 그릴 수 있는데, 이를 **예측구간**이라 한다. 95% 예측구간은 얻을 수 있는 데이터의 95%를 포함하는 범위를 나타낸다. 

*신뢰구간은 모형의 파라미터, 즉 모집단의 범위이지만 예측구간은 얻을 수 있는 데이터의 범위다.* 
![img39](assets/img/DBRS_week3/img39.png)


> 결정계수

지금까지의 회귀분석 결과를 보면 알겠지만, 최소제곱법으로 데이터에 아무리 잘 들어맞는 회귀식을 구하더라도 데이터와 회귀식이 꼭 들어맞지는 않는다. 때문에 회귀식이 잘 들어맞는지 평가하는 지표로, **결정계수 R^2**를 자주 사용한다. 

![img40](assets/img/DBRS_week3/img40.png)

분모는 분산을, 분자는 잔차 제곱의 총합으로, 최소제곱법에서 봤던 E다. 즉, 우변 제2항은 설명되지 않고 남아 있는 잔차의 비율을 표시한다. 이 숫자를 1에서 뺌으로써 데이터에 의해 설명된 비율을 나타낸다. 

+설명변수가 1개인 1차 함수의 선형회귀에서 최소제곱법을 이용할 때, 결정계수 R^2는 x와 y사이의 피어슨 상관계수 r을 제곱한 값과 같다. 

![img41](assets/img/DBRS_week3/img41.png)

그러나 R^2는 설명변수의 개수가 늘어날수록 커지는 성질이 있어 의미 없는 설명변수를 도입하면 실제론 그렇지 않은데도 설명력이 향상된 것처럼 보일 수 있다. 
따라서 설명변수 개수 k에 따라 조정한 **조정 결정계수 R^2**를 사용하는 것이 일반적이다. 

![img42](assets/img/DBRS_week3/img42.png)


> 오차의 등분산성과 정규성

최소제곱법으로 구한 선형모형의 파라미터를 대상으로 가설검정을 시행하거나 신뢰구간을 얻기 위해서는, 오차항의 확률분포가 평균=0, 분산=시그마^2인 정규분포라고 가정해야 한다. 
이는 잔차의 정규성을 샤피로-윌크 검정을 통해 확인하면 알 수 있다. 

덧붙여 이런 오차항 epsilon이 정규분포를 보이는 모형은, 8장에서 등장하는 일반화선형모형에서 오차의 확률분포을 정규분포로 한 모형과 일치한다. 

x의 값에 따라 오차 epsilon의 분산이 변하지 않는다고 가정하면, 최소제곱법에 따른 선형회귀모형의 추정으로 최량선형비편향추정량을 얻을 수 있다. 

등분산성을 확인하려면 x가 변할 때 잔차의 분산이 달라지는지를 조사하는 브루쉬-페이건 검정을 이용한다. 

![img43](assets/img/DBRS_week3/img43.png)


> 설명변수와 반응변수

분석을 하기 전, 무엇을 설명변수로 하고 무엇을 반응변수로 할 것인가를 목적에 맞게 생각해야 한다. 

1. 한쪽 변수로 다른 한쪽 변수를 설명하고자 할 때:
설명하는 쪽을 설명변수로, 설명의 대상을 반응변수로 설정한다. 

2. 인과효과를 알고 싶을 때:
원인을 설명변수로, 결과를 반응변수로 설정한다. 서로 다른 x에 대해 그 밖의 요인이 같아야만 한다. 따라서 설명변수를 무작위로 할당한 개입 실험에서 얻은 데이터나, 상정할 수 있는 다른 요인도 포함한 다중회귀모형 등을 이용하는 것이 필요하다. 

3. 데이터를 예측하고 싶을 때:
예측의 근거가 될 변수를 설명변수, 예측하고자 하는 변수를 반응변수로 설정한다. x->y의 인과관계가 없더라도 예측에는 문제가 없지만 언제든 해석 가능한 것은 아님을 주의하자. 







